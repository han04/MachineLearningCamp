---
title: 逻辑回归的损失函数
date: 2018-10-18 00:28:27
tags: 
mathjax: true
---
### **Logistic Resgression 逻辑回归**

  逻辑回归是机器学习中的一个非常常见的模型， 逻辑回归模型其实仅在线性回归的基础上，套用了一个逻辑函数。

  逻辑回归可以看做是两步，第一步和线性回归模型的形式相同，即一个关于输入x的线性函数: $Z={W}^T+ b$

第二步通过一个逻辑函数，即sigmoid函数，将线性函数转换为非线性函数。

$Z={W}^T+ b$   $\sigma(z)=\frac{1}{1+e^-x}$

### **Cost Function 损失函数**

  为了训练逻辑回归模型的参数w和b需要一个代价函数，算法的代价函数是对m个样本的损失函数求和然后除以m：

$L(\vec{y},y) = -ylog(\vec{y})-(1-y)log(1-\vec{y})$

$J(w,b)=\frac{1}{m}\sum_{i=1}^m-ylog(\vec{y}^{(i)})-(1-y^{(i)})log(1-\vec{y}^{(i)})$

### **为什么逻辑回归的损失函数是这样的形式？**

$p(y=1\mid x)=h_{\omega}(x)$

$p(y=0 | x) = 1 - h_{\omega} (x)$


  而对于上面的两个表达式，通过观察，我们发现，可以将其合并为以下表达式：

 $p(y | x) = h_{\omega} (x)^y (1-h_{\omega} (x))^{1-y}$

可以发现，在y=1时公式右边等于y^，在y=1时公式右边等于1-y^。由于log函数是严格递增函数，所以最大化log等价于最大化原函数，上式因此可以化简为式子，也就是损失函数的负数。

$L(\omega)= \prod_(i=1) ^ {m}p(y_i |x_i;\omega)= \prod_{i=1}^{m}h_{\omega} (x_i)^{y_i}(1-h_{\omega}(x_i)^{1-y_i})$

  因此，损失函数可以通过最小化负的似然函数得到，即下式：

$J(\omega) = - \frac{1}{m} \sum_{i=1}^m{y_i \log h_{\omega}(x_i)+(1-y_i \log(1-h_{\omega}(x_i)))}$
