---
title: 逻辑回归的损失函数
date: 2018-10-18T00:28:27.000Z
tags: null
mathjax: true
---  
  
### **Logistic Resgression 逻辑回归**
  
  
  逻辑回归是机器学习中的一个非常常见的模型， 逻辑回归模型其实仅在线性回归的基础上，套用了一个逻辑函数。
  
  逻辑回归可以看做是两步，第一步和线性回归模型的形式相同，即一个关于输入x的线性函数: <img src="https://latex.codecogs.com/gif.latex?Z={W}^T+%20b"/>
  
第二步通过一个逻辑函数，即sigmoid函数，将线性函数转换为非线性函数。
  
<img src="https://latex.codecogs.com/gif.latex?Z={W}^T+%20b"/>   <img src="https://latex.codecogs.com/gif.latex?&#x5C;sigma(z)=&#x5C;frac{1}{1+e^-x}"/>
  
### **Cost Function 损失函数**
  
  
  为了训练逻辑回归模型的参数w和b需要一个代价函数，算法的代价函数是对m个样本的损失函数求和然后除以m：
  
<img src="https://latex.codecogs.com/gif.latex?L(&#x5C;vec{y},y)%20=%20-ylog(&#x5C;vec{y})-(1-y)log(1-&#x5C;vec{y})"/>
  
<img src="https://latex.codecogs.com/gif.latex?J(w,b)=&#x5C;frac{1}{m}&#x5C;sum_{i=1}^m-ylog(&#x5C;vec{y}^{(i)})-(1-y^{(i)})log(1-&#x5C;vec{y}^{(i)})"/>
  
### **为什么逻辑回归的损失函数是这样的形式？**
  
  
<img src="https://latex.codecogs.com/gif.latex?p(y=1&#x5C;mid%20x)=h_{&#x5C;omega}(x)"/>
  
<img src="https://latex.codecogs.com/gif.latex?p(y=0%20|%20x)%20=%201%20-%20h_{&#x5C;omega}%20(x)"/>
  
  
  而对于上面的两个表达式，通过观察，我们发现，可以将其合并为以下表达式：
  
 <img src="https://latex.codecogs.com/gif.latex?p(y%20|%20x)%20=%20h_{&#x5C;omega}%20(x)^y%20(1-h_{&#x5C;omega}%20(x))^{1-y}"/>
  
可以发现，在y=1时公式右边等于y^，在y=1时公式右边等于1-y^。由于log函数是严格递增函数，所以最大化log等价于最大化原函数，上式因此可以化简为式子，也就是损失函数的负数。
  
<img src="https://latex.codecogs.com/gif.latex?L(&#x5C;omega)=%20&#x5C;prod_(i=1)%20^%20{m}p(y_i%20|x_i;&#x5C;omega)=%20&#x5C;prod_{i=1}^{m}h_{&#x5C;omega}%20(x_i)^{y_i}(1-h_{&#x5C;omega}(x_i)^{1-y_i})"/>
  
  因此，损失函数可以通过最小化负的似然函数得到，即下式：
  
<img src="https://latex.codecogs.com/gif.latex?J(&#x5C;omega)%20=%20-%20&#x5C;frac{1}{m}%20&#x5C;sum_{i=1}^m{y_i%20&#x5C;log%20h_{&#x5C;omega}(x_i)+(1-y_i%20&#x5C;log(1-h_{&#x5C;omega}(x_i)))}"/>
  